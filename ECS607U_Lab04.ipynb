{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnav-exe/Data-Mining-Labs/blob/main/ECS607U_Lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hox4fEXzdP-"
      },
      "source": [
        "# Lab session 4: Data Warehousing and On-line Analytical Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIVISJEEzdQA"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab (Lab session 4) is for students to get experience with **Data Warehousing** and **On-line Analytical Processing (OLAP)** covered in lecture 5, and more specifically with the concepts of **data cubes**, **data cube measures**, **typical OLAP operations**, and **data cube computation**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1qCdQp2zdQE"
      },
      "source": [
        "## 1. Introduction to Cubes\n",
        "\n",
        "This chapter describes step-by-step how to use Cubes (http://cubes.databrewery.org/), a lightweight Python framework and set of tools for development of reporting and analytical applications, Online Analytical Processing (OLAP), multidimensional analysis and browsing of aggregated data. We will be working with v1.1 of Cubes. Cubes features:\n",
        "- a logical view of analysed data - how analysts look at data, how they think of data, not not how the data are physically implemented in the data stores\n",
        "- OLAP and aggregated browsing (default backend is for relational databse - ROLAP)\n",
        "- hierarchical dimensions (attributes that have hierarchical dependencies, such as category-subcategory or country-region)\n",
        "- multiple hierarchies in a dimension\n",
        "- localizable metadata and data (see Localization)\n",
        "- authentication and authorization of cubes and their data\n",
        "- pluggable data warehouse – plug-in other cube-like (multidimensional) data sources\n",
        "\n",
        "Cubes is meant to be used by application builders that want to provide analytical functionality. Cubes also relies on methods from SQLAlchemy (https://www.sqlalchemy.org/), an open-source SQL toolkit and object-relational mapper for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9szp7SwgzdQG"
      },
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "The example data used here is the International Bank for Reconstruction and Development (IBRD) Balance Sheet. For this example we will be using the CSV file \"IBRD_Balance_Sheet__FY2010.csv\" which is provided in the supplementary material for the lab. The CSV file includes records which are characterised by a Category (and subcategories), Line Item, Fiscal Year, and Amount (in US$ millions). We first start with imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXIBCAyPzdQG",
        "outputId": "0eab9cc5-ad9e-4881-b4c0-60b1d7c141d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If you are using Google Colab, you would need to run the below line to install Cubes.\n",
        "# You can comment the below line if you are running a local python installation with Cubes installed.\n",
        "!pip install cubes\n",
        "!pip install sqlalchemy==1.3.20\n",
        "from sqlalchemy import create_engine\n",
        "from cubes.tutorial.sql import create_table_from_csv"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cubes in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from cubes) (2.8.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from cubes) (4.19.1)\n",
            "Requirement already satisfied: expressions>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from cubes) (0.2.3)\n",
            "Requirement already satisfied: grako>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from expressions>=0.2.3->cubes) (3.99.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->cubes) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->cubes) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->cubes) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->cubes) (0.10.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->cubes) (1.16.0)\n",
            "Requirement already satisfied: sqlalchemy==1.3.20 in /usr/local/lib/python3.10/dist-packages (1.3.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwzYQ-RWzdQH"
      },
      "source": [
        "We can now load the data, create a table and populate it with contents of the CSV file. Note the categories and subcategories created; check the CSV file to link the below commands with the contents of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxMa8Kp6zdQH"
      },
      "source": [
        "engine = create_engine('sqlite:///data.sqlite')\n",
        "create_table_from_csv(engine,\n",
        "                      \"IBRD_Balance_Sheet__FY2010.csv\",\n",
        "                      table_name=\"ibrd_balance\",\n",
        "                      fields=[\n",
        "                          (\"category\", \"string\"),\n",
        "                          (\"category_label\", \"string\"),\n",
        "                          (\"subcategory\", \"string\"),\n",
        "                          (\"subcategory_label\", \"string\"),\n",
        "                          (\"line_item\", \"string\"),\n",
        "                          (\"year\", \"integer\"),\n",
        "                          (\"amount\", \"integer\")],\n",
        "                      create_id=True\n",
        "                     )"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTFmeAwPzdQJ"
      },
      "source": [
        "## 3. Creating a data cube\n",
        "\n",
        "Everything in Cubes happens in an *analytical workspace*. It contains cubes, maintains connections to the data stores (with cube data), provides connection to external cubes and more.\n",
        "\n",
        "The workspace properties are specified in a configuration file slicer.ini (default name). The first thing we have to do is to specify a data store – a database which will host the cube’s data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZyn9s8mzdQK"
      },
      "source": [
        "from cubes import Workspace\n",
        "\n",
        "workspace = Workspace()\n",
        "workspace.register_default_store(\"sql\", url=\"sqlite:///data.sqlite\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9EIGCjXzdQL"
      },
      "source": [
        "The structure of data cubes (in terms of dimensions, measures, and aggregates) is specified in JSON files. We now import file 'tutorial_model.json' (found in the lab supplementary material) which includes an example model of the data cube, dimension tables, and aggregate functions for the CSV file we loaded previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnWunsqgzdQM"
      },
      "source": [
        "workspace.import_model(\"tutorial_model.json\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA01sBgQzdQN"
      },
      "source": [
        "**Please make sure to inspect the structure of the above JSON file**\n",
        "\n",
        "We can now create a data cube based on the above data cube model and data table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8R-iLAYzdQO"
      },
      "source": [
        "cube = workspace.cube(\"ibrd_balance\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd9DjXhgzdQP"
      },
      "source": [
        "## 4. Aggregations and OLAP operations\n",
        "\n",
        "*Browser* is an object that does the actual aggregations and other data queries for a cube. To obtain one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnDst0CczdQP"
      },
      "source": [
        "browser = workspace.browser(cube)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP8BLLfIzdQQ"
      },
      "source": [
        "We can now compute aggregates of the data cube as specified by the data cube model. For computing the total count of records:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOz_da0SzdQQ",
        "outputId": "46c7a88f-2bb5-4b20-83c8-cd95b65ad295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = browser.aggregate()\n",
        "result.summary[\"record_count\"]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception during reset or similar\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 543, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n",
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception closing connection <sqlite3.Connection object at 0x7948b3d7b740>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 543, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
            "    self._dialect.do_close(connection)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 549, in do_close\n",
            "    dbapi_connection.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKWET-J2zdQR"
      },
      "source": [
        "For computing a sum of the amount:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MsOxTMzzdQR",
        "outputId": "6b1bb99d-861d-48ea-e753-f431adf5a4b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result.summary[\"amount_sum\"]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1116860"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smFYyUDbzdQR"
      },
      "source": [
        "Now we can try to compute aggregates by year:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAM-7DfzdQR",
        "outputId": "8473535e-2fc6-466f-8427-d597e7851d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = browser.aggregate(drilldown=[\"year\"])\n",
        "for record in result:\n",
        "    print(record)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'year': 2009, 'amount_sum': 550840, 'record_count': 31}\n",
            "{'year': 2010, 'amount_sum': 566020, 'record_count': 31}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGSTe7jzdQS"
      },
      "source": [
        "Or compute aggregates by item category:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz2KrmECzdQS",
        "outputId": "d3d88215-10c6-4c75-badd-49dbf9034ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = browser.aggregate(drilldown=[\"item\"])\n",
        "for record in result:\n",
        "    print(record)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 558430, 'record_count': 32}\n",
            "{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 77592, 'record_count': 8}\n",
            "{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 480838, 'record_count': 22}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyNipccozdQS"
      },
      "source": [
        "We can also perform *slicing* and *dicing* operations on the data cube. The below example performs a slicing operation on the data cube by selecting only entries with the year being 2009, and displays aggregates according to the item category. Here, a *cell* defines a point of interest – portion of the cube to be aggergated or browsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGth6V1kzdQS",
        "outputId": "0e4eb228-2143-4885-a818-8728d0a9da51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cubes as cubes\n",
        "cuts = [cubes.PointCut(\"year\", [\"2009\"])]\n",
        "cell = cubes.Cell(cube, cuts)\n",
        "result = browser.aggregate(cell, drilldown=[\"item\"])\n",
        "for record in result:\n",
        "    print(record)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 275420, 'record_count': 16}\n",
            "{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 40037, 'record_count': 4}\n",
            "{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 235383, 'record_count': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxuBWl9azdQS"
      },
      "source": [
        "It's worth noting that in Cubes, slicing operations can be created by either specifying a \"point cut\" which selects a single value of an attribute in a given dimension (called using the cubes.PointCut() function as above), or by specifying a \"range cut\", which selects a range of values for a given dimension. The range cut can be called using the cubes.RangeCut() function, which takes as input the attribute name, the minimum value of the specified range, and the maximum value of the range.\n",
        "\n",
        "Similarly, we can perform a *dicing* operation on the data cube by performing a selection on two or more dimensions. The below example performs a dicing operation on the data cube, selecting entries with the year being 2009 and the item category being \"a\", and displays the aggregate results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwoOriQizdQT",
        "outputId": "4172899c-0917-4a1d-c2ec-0561a23f3a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n",
        "cell = cubes.Cell(cube, cuts)\n",
        "result = browser.aggregate(cell,drilldown=[\"item\"])\n",
        "result.summary"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amount_sum': 275420, 'record_count': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZkEpxrCzdQT"
      },
      "source": [
        "We can also *drill down* lower in the Category hierarchy. Here, we perform a dicing operation to select records with year being 2009 and item category being \"a\" (corresponding to assets), and show aggregates for each subcategory level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWI2aH9yzdQT",
        "outputId": "ce209566-fea1-4543-9302-78a98eb6dcb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n",
        "cell = cubes.Cell(cube, cuts)\n",
        "result = browser.aggregate(cell,drilldown=[\"item:subcategory\"])\n",
        "for record in result:\n",
        "    print(record)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'da', 'item.subcategory_label': 'Derivative Assets', 'amount_sum': 123065, 'record_count': 4}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'dfb', 'item.subcategory_label': 'Due from Banks', 'amount_sum': 3044, 'record_count': 2}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'i', 'item.subcategory_label': 'Investments', 'amount_sum': 41012, 'record_count': 1}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'lo', 'item.subcategory_label': 'Loans Outstanding', 'amount_sum': 103657, 'record_count': 1}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'nn', 'item.subcategory_label': 'Nonnegotiable', 'amount_sum': 1202, 'record_count': 1}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'oa', 'item.subcategory_label': 'Other Assets', 'amount_sum': 2247, 'record_count': 3}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'orcv', 'item.subcategory_label': 'Other Receivables', 'amount_sum': 984, 'record_count': 2}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'rcv', 'item.subcategory_label': 'Receivables', 'amount_sum': 176, 'record_count': 1}\n",
            "{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 's', 'item.subcategory_label': 'Securities', 'amount_sum': 33, 'record_count': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception during reset or similar\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 543, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n",
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception closing connection <sqlite3.Connection object at 0x7948b3d7b740>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 543, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
            "    self._dialect.do_close(connection)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 549, in do_close\n",
            "    dbapi_connection.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 133354449227776 and this is thread id 133353558763072.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "HD5btV37sTuS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9trjWTB4zdQT"
      },
      "source": [
        "**Exercise**:\n",
        "\n",
        "Using the same CSV file and data cube in the above lab tutorial: </br>\n",
        "i) modify the \"tutorial_model.json\" file to include aggregate measures for the minimum and maximum amount in the data cube; </br>\n",
        "ii) using these implemented aggregate measures, produce the values for the minimum and maximum amount in the data per year.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    }
  ]
}