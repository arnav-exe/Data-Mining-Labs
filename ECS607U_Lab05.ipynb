{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUJRrimoSfo"
      },
      "source": [
        "# Lab session 5: Classification and Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvfYNFKoSfq"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab (Lab session 5) is for students to get experience with **Classification** and **Clustering**, both covered in lecture 6, by using common Python libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91N-pqHAoSft"
      },
      "source": [
        "## Classification\n",
        "\n",
        "In order to present functionalities for data classification and clustering in Python, we use the [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/) as part of a working example.\n",
        "\n",
        "The dataset is stored in a file called ``mnist.pkl``, which is serialized by [pickle](https://docs.python.org/3/library/pickle.html). \n",
        "\n",
        "In the snippet below, ``X`` is a matrix (numpy array) where each row corresponds to an observation and each column corresponds to a feature. Each observation is the result of *flattening* a 28 x 28 grayscale image of a handwritten digit into a vector. The list ``y`` (numpy array) contains the class (a digit from 0 to 9) corresponding to each observation (row) in the matrix ``X``.\n",
        "\n",
        "For the sake of this tutorial, we select only the first 2000 observations (rows) from the original MNIST training dataset to expedite computations.\n",
        "\n",
        "It is possible to use the ``matplotlib`` function ``imshow`` to visualise any observation by reshaping it appropriately. You may change the index ``i`` to select an observation for visualisation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWFU5IvooSfu"
      },
      "source": [
        "# Configuring the appearance of ``seaborn`` graphics in this notebook\n",
        "%config InlineBackend.figure_formats = set(['retina'])\n",
        "\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Selecting the training data from the original dataset\n",
        "f = open('mnist.pkl', 'rb')\n",
        "X, y = pickle.load(f, encoding='latin1')[0]\n",
        "f.close()\n",
        "\n",
        "# Subsampling\n",
        "sample_size = 2000\n",
        "X, y = X[:sample_size], y[:sample_size]\n",
        "\n",
        "i = 10\n",
        "plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.show()\n",
        "print('Observation index: {0}. Class: {1}.'.format(i, y[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOP0Z62foSfv"
      },
      "source": [
        "The library ``sklearn`` implements all the learning algorithms that we covered during the lectures. Each algorithm is implemented by a class that provides a standard interface.\n",
        "\n",
        "The class ``KNeighborsClassifier`` implements a k-nearest neighbour classification algorithm. The number of neighbours is specified by the constructor parameter ``n_neighbors``.\n",
        "\n",
        "The method ``KNeighborsClassifier.fit`` is responsible for learning a classifier for a dataset represented by an observation matrix and a class array. This is precisely how our data is represented.\n",
        "\n",
        "The method ``KNeighborsClassifier.score`` computes the accuracy of a classifier on a specific dataset. This method must be called after ``KNeighborsClassifier.fit``.\n",
        "\n",
        "As we have seen, a one-nearest neighbour classifier always predicts the correct class for any observation that already exists in a dataset (as long as there are no equal observations with different classes), which is why we obtain 100% accuracy on the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_r1mctJoSfw"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X, y)\n",
        "print('Training dataset accuracy: {0}.'.format(knn.score(X, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPxqSYP4oSfw"
      },
      "source": [
        "In order to evaluate the capacity of a k-nearest neighbour classifier to assign unseen observations to correct classes, it is necessary to split the original dataset into a training dataset and a test dataset.\n",
        "\n",
        "The ``sklearn`` function ``train_test_split`` can be used for this purpose. This function selects a fraction of the observations (``test_size``) to compose the test set. The parameter ``random_state`` allows reproducibility by fixing the seed used by this (pseudo)random selection procedure. \n",
        "\n",
        "The function ``train_test_split`` returns a training observation matrix, a test observation matrix, a training class array, and a test class array. From now on, all classifiers are trained without access to the test dataset.\n",
        "\n",
        "The method ``KNeighborsClassifier.fit`` can be used to fit a k-nearest neighbour classifier to the training dataset. Naturally, the accuracy of the one-nearest neighbour classifier on the training set remains 100%. The accuracy on the test set, however, is a better estimate of the performance of the classifier on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4run65xSoSfx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "print('Training dataset accuracy: {0}.'.format(knn.score(X_train, y_train)))\n",
        "print('Test dataset accuracy: {0}.'.format(knn.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm92SlgoSfx"
      },
      "source": [
        "After a ``KNeighborsClassifier`` is fit, the method ``KNeighborsClassifier.predict`` can be used to predict the classes for a list of observations.\n",
        "\n",
        "For example, we may use this method to predict the class for every observation in the test dataset. This allows obtaining the indices of the observations that are classified incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I47mfrtRoSfy"
      },
      "source": [
        "y_pred = knn.predict(X_test)\n",
        "mistakes = np.nonzero(y_pred != y_test)[0]\n",
        "\n",
        "print('Indices of misclassified observations:')\n",
        "print(mistakes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9NKQAaoSfy"
      },
      "source": [
        "We can use the ``matplotlib`` function ``imshow`` to visualise a misclassified observation. You may change the index for ``mistakes`` in order to select an observation for visualisation (requires running all cells above, because ``y_pred`` is modified later). Note how some misclassifications are understandable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SRnYZXRoSfz"
      },
      "source": [
        "i = mistakes[13]\n",
        "plt.imshow(X_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.show()\n",
        "print('Observation index: {0}. Class: {1}. Prediction: {2}.'.format(i, y_test[i], y_pred[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkO2w2QloSfz"
      },
      "source": [
        "Some datasets need to be preprocessed before a learning algorithm can be applied. The training dataset must be preprocessed without access to the test dataset, otherwise the resulting test accuracy after preprocessing is likely to overestimate the accuracy on unseen data.\n",
        "\n",
        "The function ``make_pipeline`` allows chaining a sequence of preprocessing operations (such as individually standardizing each feature, which is accomplished by the class ``StandardScaler``) before a learning algorithm is applied. This function returns a ``Pipeline`` object that can be used to fit, predict, and score, just as any other classification algorithm in ``sklearn``. A ``Pipeline`` preprocesses incoming data appropriately before making predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIBcVR9ToSfz"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=1))\n",
        "\n",
        "knn_pipe.fit(X_train, y_train)\n",
        "print('Test dataset accuracy: {0}.'.format(knn_pipe.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ1Asa_uoSf0"
      },
      "source": [
        "Consider the task of selecting the best possible number of neighbours k (a hyperparameter) for the k-nearest neighbour classification algorithm.\n",
        "\n",
        "As we have seen, a hyperparameter should not be chosen based on its performance on the test dataset. In that case, there would be no data left to enable a reliable estimate of how well this choice generalizes to unseen data. After all, this choice would have been based on the test dataset.\n",
        "\n",
        "Therefore there exist two options: either the whole dataset is split into a training, a validation and a testing set, or k-fold cross validation is performed. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "29jOl89Xfo6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Option 1*: **Exercise**\n",
        "\n",
        "Split the whole dataset into training, validation and testing sets with ratios 65-15-20% and then perform  k-nearest neighbour with k = 5 and 15. Choose the best value for k according to the performance on the validation set.*italicized text*"
      ],
      "metadata": {
        "id": "TNswLeBffATB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert code here"
      ],
      "metadata": {
        "id": "yUa8A3HYfmez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "CfQDN9QwfqP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2: The function ``cross_validate`` conducts K-fold cross validation using a specified training dataset. Cross-validation involves splitting a training dataset into K folds of equal size (this K and the k in k-nn should not be confused; they are different). Each fold is used as a validation set (on which the performance is computed) when the remaining folds are used as an effective training set (where a classifier is learned). The parameter ``cv`` controls the number of folds K. The function returns a dictionary with statistics that include a list of accuracies (i.e. performance metric) that can be accessed by the keyword ``'test_score'``.\n",
        "\n",
        "After a hyperparameter is selected by cross-validation, it can be used to fit a classifier to the entire training dataset, which can then be evaluated in the test dataset.\n",
        "\n",
        "Once again, if any step leading to model selection is based on the resulting performance on the test dataset, the resulting test accuracy is likely to overestimate the accuracy of the model on unseen data."
      ],
      "metadata": {
        "id": "53SzsxyEeiM2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeBHf_OeoSf1"
      },
      "source": [
        "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_15 = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Note that the data was already implicitly shuffled by train_test_split\n",
        "result_5 = cross_validate(knn_5, X_train, y_train, cv=5)\n",
        "result_15 = cross_validate(knn_15, X_train, y_train, cv=5)\n",
        "\n",
        "print('Average accuracy across folds (k = 5): {0}.'.format(result_5['test_score'].mean()))\n",
        "print('Average accuracy across folds (k = 15): {0}.'.format(result_15['test_score'].mean()))\n",
        "      \n",
        "knn_5.fit(X_train, y_train)\n",
        "print('Test dataset accuracy (k = 5): {0}.'.format(knn_5.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhERgJSDoSf1"
      },
      "source": [
        "The class ``GridSearchCV`` offers a convenient way to choose hyperparameters based on cross-validation. Its constructor receives a classification algorithm object (such as a ``KNeighborsClassifier`` object) and a dictionary that maps hyperparameter names to lists of values that should be considered. Once the method ``GridSearchCV.fit`` is called, each possible combination of hyperparameters from each of the lists is evaluated using cross-validation. The best hyperparameter setting is then used to fit a classifier to the training dataset. After fitting, the ``GridSearchCV`` object can be used to predict and score just as any other classification algorithm in ``sklearn``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59I1PvY2oSf2"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn_cv = GridSearchCV(knn, parameters, cv=5)\n",
        "knn_cv.fit(X_train, y_train)\n",
        "\n",
        "print('Best hyperparameter setting: {0}.'.format(knn_cv.best_estimator_))\n",
        "print('Average accuracy across folds of best hyperparameter setting: {0}.'.format(knn_cv.best_score_))\n",
        "print('Test dataset accuracy of best hyperparameter setting: {0}.'.format(knn_cv.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ifj8XHloSf2"
      },
      "source": [
        "Accuracy is not the only classification performance metric available on ``sklearn``. For instance, it is possible to compute the precision and recall for each class by considering the remaining classes as beloging to a single (negative) class. This is accomplished by the function ``precision_recall_fscore_support``.\n",
        "\n",
        "The function ``confusion_matrix`` creates a confusion matrix given a class/label array and a prediction array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZzK_ybKoSf3"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = knn_cv.predict(X_test)\n",
        "\n",
        "precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred)\n",
        "print('Precision for each class: {0}.'.format(precision))\n",
        "print('Recall for each class: {0}.\\n'.format(recall))\n",
        "\n",
        "df = pd.DataFrame.from_records(confusion_matrix(y_test, y_pred))\n",
        "print('Confusion matrix:')\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kejHZvqtoSf3"
      },
      "source": [
        "As mentioned before, ``sklearn`` implements learning algorithms covered during the lectures:  [support vector machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [artificial neural networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), [decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and [random forest classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "The interface provided by each of these learning algorithms is analogous to the interface provided by the k-nearest neighbour classification algorithm that we have used so far.\n",
        "\n",
        "For instance, in order to train a random forest classifier composed of 100 decision trees, an object of the class ``RandomForestClassifier`` can be constructed with ``n_estimators`` set to 100. This object can be used to fit the training dataset and compute the accuracy on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNoTJBItoSf3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rfc.fit(X_train, y_train)\n",
        "print(rfc.score(X_test, y_test))\n",
        "print('Test dataset accuracy (random forest classifier): {0}.'.format(rfc.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "mrsOpM__rekf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise##\n",
        "\n",
        "Train a support vector machine classifier using the same training dataset used in the previous sections and compute its accuracy on the corresponding test dataset. You can use the default hyperparameters for the class SVC from sklearn.svm"
      ],
      "metadata": {
        "id": "uHnZK8ZGqzh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert code here"
      ],
      "metadata": {
        "id": "4GJw77--3RGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "H1smir3rrgbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise##\n",
        "\n",
        "\n",
        "Using the same training dataset used in the previous sections, employ ``GridSearchCV`` to find the best hyperparameter settings based on 5-fold cross-validation for a ``RandomForestClassifier``. Consider ``n_estimators`` $ \\in \\{ 50, 100, 200\\}$ and ``max_features`` $ \\in \\{0.1, 0.25\\}$. Use the default values for the remaining hyperparameters. Compute the accuracy of the best model on the corresponding test dataset. "
      ],
      "metadata": {
        "id": "9gcT0yGR2_EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert code here"
      ],
      "metadata": {
        "id": "fdB2oTI33Sha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "</br>"
      ],
      "metadata": {
        "id": "MaXtgWsM3PLP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQe9zr1EoSf3"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "The clustering algorithms implemented by ``sklearn`` have an interface that is similar to the interface for the classification algorithms.\n",
        "\n",
        "The class ``KMeans`` implements the k-means clustering algorithm. The number of clusters ``n_clusters`` is a parameter for the constructor of this class. \n",
        "\n",
        "The method ``KMeans.fit_predict`` is equivalent to a call to ``KMeans.fit`` followed by a call to ``KMeans.predict``, which is responsible for attributing each observation to a cluster. After the clustering is computed by a call to ``KMeans.fit``, the sum of squared errors of the clustering is available as a class variable named ``inertia_``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOd-hVO0oSf4"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "print('Sum of squared errors (k = 10): {0}.'.format(kmeans.inertia_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLtJPXlvoSf4"
      },
      "source": [
        "In order to compare the quality of clusterings of the same dataset for different numbers of clusters, it is possible to employ both sums of squared errors and silhouette coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mjxY-EyqoSf4"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max_k = 15\n",
        "\n",
        "# Sum of squared errors for each k\n",
        "sses = []\n",
        "\n",
        "# Silhouette coefficient for each k\n",
        "silhouettes = []\n",
        "\n",
        "for k in range(2, max_k + 1):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    y_pred = kmeans.fit_predict(X)\n",
        "    \n",
        "    sses.append(kmeans.inertia_)\n",
        "    silhouettes.append(silhouette_score(X, y_pred))\n",
        "\n",
        "# Plotting sums of squared errors\n",
        "df = pd.DataFrame({'sum of squared errors': sses, 'number of clusters': list(range(2, max_k + 1))})\n",
        "sns.lineplot(x='number of clusters', y='sum of squared errors', data=df)\n",
        "plt.xticks(df['number of clusters'])\n",
        "plt.show()\n",
        "\n",
        "# Plotting silhouette coefficients\n",
        "df = pd.DataFrame({'silhouette coefficient': silhouettes, 'number of clusters': list(range(2, max_k + 1))})\n",
        "sns.lineplot(x='number of clusters', y='silhouette coefficient', data=df)\n",
        "plt.xticks(df['number of clusters'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VjCYNYoSf4"
      },
      "source": [
        "A projection computed using t-stochastic neighbor embedding (implemented by the class ``TSNE``) can be used to visualise the results of clustering. \n",
        "\n",
        "Recall that dimensionality reduction attemps to represent a dataset by a projection such that each point in the projection corresponds to an observation. The t-stochastic neighbor embedding algorithm attempts to preserve neighbourhoods of the dataset in the corresponding projection.\n",
        "\n",
        "It is important to note that a projection is not, in general, a reliable representation of a dataset. Therefore, care must be taken before deriving conclusions from such visualisations. \n",
        "\n",
        "Note that the numbers assigned to clusters are unlikely to match the numbers assigned to classes. Therefore, only relative colouring is important in the plots shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiUlOCF-oSf5"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Computing a projection using t-stochastic neighbour embedding\n",
        "embedding = TSNE(n_components=2, perplexity=50, random_state=0)\n",
        "Xp = embedding.fit_transform(X)\n",
        "\n",
        "# Plotting projection colored by classes\n",
        "df_projection = pd.DataFrame({'x': Xp[:, 0], 'y': Xp[:, 1], 'class': y})\n",
        "sns.scatterplot(x='x', y='y', hue='class', palette=sns.color_palette(), data=df_projection)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "# Plotting projection colored by clusters\n",
        "df_projection = pd.DataFrame({'x': Xp[:, 0], 'y': Xp[:, 1], 'cluster': y_pred})\n",
        "sns.scatterplot(x='x', y='y', hue='cluster', palette=sns.color_palette(), data=df_projection)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}