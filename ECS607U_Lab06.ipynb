{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WegGwHz10QEf"
      },
      "source": [
        "# Lab session 6: Association Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZUOd9_x0QEg"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab (Lab session 6) is for students to get experience with **Association Analysis** covered in week 8, by using typical Python libraries.\n",
        "\n",
        "\n",
        "This session starts with a tutorial that uses examples to introduce you to  practical knowledge. We highly recommend that you read the following tutorials if you need a gentler introduction to the libraries that we use:\n",
        "- [Mlxtend: Apriori](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)\n",
        "- [Numpy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html)\n",
        "- [Numpy: basic broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
        "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
        "- [Matplotlib](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
        "- [Seaborn](https://seaborn.pydata.org/tutorial/relational.html)\n",
        "- [Scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poBjNRF80QEi"
      },
      "source": [
        "## 1. Frequent itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2GJqWCp0QEj"
      },
      "source": [
        "In order to present functionalities for association analysis in Python, we adapt an example from the ``mlxtend`` documentation.\n",
        "\n",
        "Consider a dataset composed of five transactions. This dataset is represented by a list of five elements, each of which is a list of items bought during a trip to a supermarket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfbulGiF0QEj"
      },
      "source": [
        "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
        "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
        "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
        "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
        "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbaY6ZDN0QEj"
      },
      "source": [
        "The library ``mlxtend`` requires that each transaction is represented by a binary vector where each element indicates the presence or absence of a specific item.\n",
        "\n",
        "The method ``TransactionEncoder.fit_transform`` can be used to convert the dataset created above into this expected format. This method returns a binary matrix (numpy array) where each transaction corresponds to a row and each column corresponds to an item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVRaQhnB0QEk"
      },
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(dataset)\n",
        "print(te_ary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwX4i0H50QEk"
      },
      "source": [
        "The item corresponding to each column is stored by the ``TransactionEncoder`` object in a variable called ``columns_``. This variable can be used to create a ``DataFrame`` that conveniently represents the transaction dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJVGL2P0QEk"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIY7Qi5j0QEl"
      },
      "source": [
        "The ``mlxtend`` function ``apriori`` receives a ``DataFrame`` that represents a transaction dataset and a parameter that specifies the support threshold. This function returns a ``DataFrame`` that contains one row for each frequent itemset. Each row contains a python ``frozenset`` that represents the itemset (by column indices) and a number that represents the support of this itemset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_44Ffbfg0QEm"
      },
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=0.6)\n",
        "display(frequent_itemsets)\n",
        "\n",
        "itemset = frequent_itemsets.loc[5]\n",
        "print('Itemset: {0}. Support: {1}.'.format(itemset['itemsets'], itemset['support']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8cwFrys0QEm"
      },
      "source": [
        "Conveniently, if the parameter ``use_colnames`` is set to ``True``,  the ``mlxtend`` function ``apriori`` may instead return a ``DataFrame`` that represents itemsets by ``frozensets`` of item names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR2yUWTT0QEn"
      },
      "source": [
        "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
        "display(frequent_itemsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--cxPvfs0QEn"
      },
      "source": [
        "Using typical ``pandas`` functionalities, it is easy to include a column in such a ``DataFrame`` to register the number of items in each frequent itemset, which can be used to filter itemsets by length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5tvWSmD0QEo"
      },
      "source": [
        "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x)) # length of each frozenset\n",
        "print('Frequent 3-itemsets:')\n",
        "display(frequent_itemsets[frequent_itemsets['length'] == 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSdsp-q80QEo"
      },
      "source": [
        "It is also easy to create a ``dict`` that maps any frequent itemset (represented by a ``frozenset``) to its support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjKQLrej0QEo"
      },
      "source": [
        "support = {}\n",
        "for _, row in frequent_itemsets.iterrows():\n",
        "    support[row['itemsets']] = row['support']\n",
        "\n",
        "itemset = frozenset(['Onion', 'Eggs'])\n",
        "print('Itemset: {0}. Support: {1}.'.format(itemset, support[itemset]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDnGEz1z0QEo"
      },
      "source": [
        "## 2. Association rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bb-Ohe70QEp"
      },
      "source": [
        "The ``mlxtend`` function ``association_rules`` receives a ``DataFrame`` that represents the set of frequent itemsets and returns a ``DataFrame`` that represents strong association rules for a specified confidence threshold. Each row in the resulting  ``DataFrame`` contains an association rule together with some potentially useful measures (we have not covered lift, leverage, or conviction). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCAnce4q0QEp"
      },
      "source": [
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "strong_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "display(strong_rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n"
      ],
      "metadata": {
        "id": "4IDliL4-8jmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample dataset to be used is one of Kaggleâ€™s e-commerce resources containing historical sales transactions of electronic items. The dataset can be found in QM+.\n",
        "\n",
        "The tasks to be completed for this exercise are:\n",
        "\n",
        "1) Import the dataset </br>\n",
        "2) Keep only the columns: 'Order ID', 'Product' </br>\n",
        "3) Reset the index of the dataframe </br>\n",
        "4) Use the following command for structuring the dataframe into a form/format for analysis: </br> pivot_table(index = 'Order ID', columns = 'Product', aggfunc = 'count') </br>\n",
        "5) Check the resulting dataframe and perform a necessary pre-processing technique </br>\n",
        "6) Convert the columns to boolean </br>\n",
        "7) Identify the support frequency of items </br>\n",
        "The support frequency is the number of times any individual item is sold (here we only count each item once per order) divided by the total number of transactions (or orders in this example). Which are the top-5 most frequent selling items? Can this insight help a retailer also boost the sales of less frequent selling items by pairing them with these high frequency lower cost electronic goods? Which products would make sense to pair together given this insight? </br>\n",
        "8) Run the apriori algorithm from mlxtend </br>\n",
        "9) Compute items that sell at minimum in a percentage of all orders placed in the month (e.g. try min_support=0.00000001) </br>\n",
        "10) Compute pairs of orders that have a meaningful measure (e.g lift of more than 1; confidence of more than 0.1) in their sales when marketed together\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8XDFXDnR_ae6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# insert code"
      ],
      "metadata": {
        "id": "-6qQ9Bd89vxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}